Dado que la intención de este trabajo consiste en utilizar los datos textuales
obtenidos de la sesión del Congreso en cuestión para realizar una selección
de rasgos a fin de entrenar distintos modelos, resulta crucial procesar dichos
datos de forma tal que el proceso de extracción de información por parte
de un algoritmo de aprendizaje automático se vea faciliado. En concreto, palabras
como `aceptaron', `acepta', `aceptó' refieren las tres a la misma acción (`aceptar'),
pero a la hora de extraer los rasgos a partir de los discursos prounciados serán
consideradas como tres rasgos distintos si no las convertimos a una forma única o
base previamente.
\par
El \textbf{lema} es la forma de una palabra que se utiliza para hacer referencia a ella
en la entrada de un diccionario y varía según la convención en cada lengua. En español
y en inglés, por ejemplo, los verbos se enuncian en infinitivo, pero en latín lo hacen
utilizando la primera persona del singular del presente en modo indicativo, la segunda
del mismo tiempo y la primera del pretérito perfecto. Por otro lado, el
\textbf{\textit{stemizado}} es el proceso por el cual se le remueven los sufijos
a una palabra, persistiendo solamente su primer o primeros morfemas y utilizando
\todo[color=pink]{Lematizado y stemmizado: consultar con FerCa}
esto como forma base\footnote{\citet[Capítulo~3]{bird2009natural}}.
\par
En este trabajo se exploaron ambas opciones de procesamiento. Cuestiones que se tuvieron
en cuenta particularmente fueron la preservación de la marcación de género, puesto que
no resulta menor respecto del tema en consideración, y la minimización de formas a las
cuales se transformaron los verbos. Para el \textit{stemizado} se utilizó la
libería NLTK\footnote{\url{https://www.nltk.org/}}, la cual proporciona distintos algoitmos posibles. Aquí, se optó por la
implementación de \textit{SnowballStemmer}\footnote{Sobre la implementación de la
librería, consultar la documentación en
\url{https://www.nltk.org/api/nltk.stem.SnowballStemmer.html}. Y, para más información
sobre el algoritmo en español, dirigirse a
\url{https://snowballstem.org/algorithms/spanish/stemmer.html}}, que permite elegir
la lengua sobre la cual se quiere trabajar. Respecto del lematizado, se recurrió
a la librería spaCy\footnote{\url{https://spacy.io/}}, que cuenta con modelos
pre-entrenados para la predicción de lemas en distintos idiomas. El modelo aquí
utilizado es el \textit{es-core-news-md}, un modelo entrenado con textos
proveniendtes de noticias y textos de los medios de
comunicación\footnote{\url{https://spacy.io/models/es\#es_core_news_md}}.
\par
En lo que refiere a la marcación de género, ninguno de los procedimientos se mostró
superiro por sobre el otro para los propósitos de este trabajo, dado que ambos
remueven la marcación de este accidente, ya sea eliminando el sufijo que lo
representa (`-o' y `-a' en la mayoría de los casos) como lo hace el
\textit{stemizado}, ya transformando la palabra completa a su forma canónica
(en masculino singular, en el español) como lo hace el lematizado. En el caso
de la conversión de los verbos a una forma base, en cambio, el lematizado permitió
una mejor reducción, dado que posibilitó transformar las distintas ocurrencias
de un verbo a una única forma: el infinitivo. Al ser un procedimiento que se
centra únicamente en la remoción de los sufijos, el \textit{stemizado} no permite
lo mismo en el caso de una lengua como el español, que presenta una considerable
variedad de verbos irregulares al nivel de la raíz\todo[color=pink]{Morfología
irregularidad: pedir referecia a FerCa}, lo que conduce que, tras el
\textit{stemizado}, las distintas ocurrencias de un mismoverbo sean reducidas
a tantas formas como irregularidades presente su raíz. Ejemplo de esto es El
verbo `aprobar', que fue transformado en `aprob', para los casos como
`aprobamos' o `aprobaron' y `aprueb', para aquellos como `aprueba',
`apruebe'. Es por esto que, si bien ambos procesamientos requirieron una revisión
y correcciómn ulteriores, se prefirió el lematizado por sobre el \textit{stemizado}
dado que permitió una transformación de palabras más clara para el español.
\par
En concreto, el flujo de trabajo consistió en procesar cada discurso emitido
con los modelos pre-entrenados de la librería spaCy, los cuales permitieron
predecir, para cada token, su lema y \textit{POS tag}\footnote{El
\textit{POS tag}, por \textit{Part of Speech}, o etiquetado gramatical consiste
en la asignación a cada palabra de su categoría sintáctica. Los modelos de la
librería spaCy emplean el sistema de etiquetas de \textit{Universal Dependencies}.
Para más información, visitar
\url{https://spacy.io/usage/linguistic-features\#pos-tagging}
y \url{https://universaldependencies.org/u/pos/}}. Luego, esta
información fue volcada en una tabla con tres columnas: una para las ocurrencias
reales de las palabras, otra para los lemas asignados y una tercera con las etiquetas
sintácticas predichas. En un paso siguiente, estas asignaciones fueron corregidas
manualemente, y los lemas y etiquetas obtenidos fueron utilizados para la
extracción de rasgos en el entrenamiento de modelos que se detalla a continuación.
Los criterios de corrección utilizados pueden encontrarse en el apéndice
\ref{appendix-annotation}, y la sección \ref{subsec-results-annotation}
proporciona un breve análisis de los resultados de este proceso.
