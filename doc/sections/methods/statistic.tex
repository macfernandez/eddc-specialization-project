Siguiendo a \cite{monroe2008fightin}, se aplic\'o una serie de t\'ecnicas
no basadas en modelos probabil\'isticos para la selecci\'on de rasgos.
Dos de los m\'etodos aplicados (\textit{TF-IDF} y \textit{Word Scores}) consisten
en algoritmos para la asignaci\'on de puntuaciones o \textit{scores} a las
palabras dentro de un \textit{corpus}. El resto de las t\'ecnicas utilizan
m\'etricas estad\'isticas sencillas. Para la aplicación de estos procedimientos
se utilizaron las lematizaciones de las palabras incluyendo su categoría
gramatical.

\subsubsection{Diferencia de frecuencias absolutas}
\label{subsubsec-methods-freq-abs}
En primer lugar, se contrastaron los lemas de los discursos de los
senadores con votos positivos conlos de los de senadores con votos negativos
utilizando la diferencia de frecuencias absolutas:

\begin{equation}
y_w = y_{w}^{(P)}-y_{w}^{(N)}
\end{equation}

donde $y_{w}^{(X)}$ refiere a la cantidad de ocurrencias absolutas del lema
\textit{w} en los discursos que votaron de manera positiva (P) y negativa (N).
En los casos en los que $y_{w}$ sea mayor o igual a $0$, el lema \textit{w}
ser\'a caracter\'istico del discurso de los senadores que votaron a favor de
la legalizaci\'on. En caso contrario, ser\'a caracter\'istica de quienes votaron
en contra.

\subsubsection{Diferencia de proporciones}
\label{subsubsec-methods-proportions}
El segundo contraste realizado utiliz\'o, en lugar de las frecuencias absolutas,
la proporci\'on de los lemas en cada conjunto de discursos.

\begin{equation}
    f_w = f_{w}^{(P)}-f_{w}^{(N)}
\end{equation}


donde $f_{w}^{(X)}$ se define como $y_{w}^{(X)} / n^{(X)}$, siendo $y_{w}^{(X)}$
la frecuencia absoluta del lema $w$ en el cojunto los discursos de $X$
y $n^{(X)}$, la cantidad de lemas totales en esos discursos.

El mismo abordaje se intent\'o removiendo previamente las palabras consideradas
\textit{stopwords}. Para esto se probaron dos alternativas. Por un lado, se
utiliz\'o el conjunto predefinido de la librer\'ia \textit{NLTK}
\footnote{\url{https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/stopwords.zip}}
y, por otro, se utilizó la Ley de Zipf. Este último procedimiento consiste en
calcular la frecuencia absoluta de aparición de
cada token y, en virtud de ella, establecer un \textit{ranking} de palabras
ordenándolas de manera decreciente según su frecuencia de aparición.
Luego, se contrasta dicha frecuencia con el orden asignado y se define
un umbral a partir del cual una palabra es considerada \textit{stopword}.

\subsubsection{Ratio de \textit{Odds}}
Asimismo, se compararon los \textit{odds} de ambos conjuntos de discursos:

\begin{equation}
    O_w = \frac{O_{w}^{(P)}}{O_{w}^{(N)}}
\end{equation}

aqu\'i, $O_{w}^{(X)}$ consiste en $f_{w}^{(X)}/(1-f_{w}^{(X)})$.

\subsubsection{Ratio \textit{Log-odds}}
Sobre los \textit{odds}, tambi\'en se calcul\'o el \textit{ratio log-odds}.
Para este c\'alculo se utiliz\'o el logaritmo natural: $\ln{O_w}$.
Asimismo, se realizó un suavizado sobre las frecuencias en los casos
en los que las palabras solo se encontraban en uno de los grupos
discursivos. Dicho suavizado consistió en agregar un peso predefinido a las
frecuencias relativas, $\tilde{f}^{i}_{w} = f^{i}_{w}+\epsilon$, y tomar
este valor en el cálculo del ratio. Siguiendo a \cite{monroe2008fightin},
se utilizó $\epsilon=0.5$.

\subsubsection{\textit{TF-IDF}}
\label{subsubsec-methods-tfidf}
Los autores del art\'iculo ``\usebibentry{monroe2008fightin}{title}'' indican
que utilizaron tambi\'en, como estad\'istica comparativa, el algoritmo de asignaci\'on
de peso \textit{TF-IDF}. Siguiendo su proceder, en este trabajo se intent\'o
hacer lo mismo. Para ello, fue necesario implementar el c\'alculo, dado que
la f\'ormula utilizada en el \textit{paper} no era la misma que la implementada
por \textit{scikit-learn}, la librer\'ia usual para extraer
este tipo de \textit{scores}.\footnote{Para m\'as informaci\'on sobre las f\'ormulas
implementadas por \textit{scikit-learn}, visitar:
\url{https://scikit-learn.org/stable/modules/feature_extraction.html\#tfidf-term-weighting}}\par
Los autores indican haber usado dos fórmulas diferentes: ambas utilizando la frecuencia
de término natural y sin normalizar, pero una con la frecuencia de documentos
con logaritmo (fórmula \ref{equation-tfidf-ntn}) y la otra, con frecuencia de documento
natural (fórmula \ref{equation-tfidf-nnn}).

\begin{equation}
\label{equation-tfidf-ntn}
    tf.idf_{w}^{(X)}(ntn) = f_{w}^{(X)} \times \ln\bigg({\frac{1}{df_{w}}}\bigg)
\end{equation}

\begin{equation}
\label{equation-tfidf-nnn}
    tf.idf_{w}^{(X)}(nnn) = \frac{f_{w}^{(X)}}{df_{w}}
\end{equation}

donde $f_{w}^{X}$ refiere a la proporci\'on del lema $w$ en el discurso
perteneciente a $X$ (con $X \in \lbrace P,N \rbrace$) y $df_w$ refiere a la
cantidad de documentos totales en los que aparece el lema $w$.
\par
Lo que los autores no mencionan es qu\'e tipo de comparaci\'on realizan entre los
lemas pertenecientes a los distintos discursos. Siguiendo los procedimientos
anteriores, en este trabajo se opt\'o por usar la sustracci\'on, por lo que:
\todo[color=pink]{en resultados, recordar hablar de lo pol\'emico de este abordaje
para}

\begin{equation*}
    tf.idf_{w} = tf.idf_{w}^{P}-tf.idf_{w}^{N}
\end{equation*}

\subsubsection{\textit{Word Scores}}
\label{subsubsec-methods-wordscores}
Por \'ultimo, se utiliz\'o tambi\'en el procedimiento \textit{WordScores} descripto por
\cite{laver2003extracting}, que le asigna un peso a los lemas a partir
del siguiente c\'alculo:\todo[color=pink]{checkear si es de ellos o de qui\'en}

\begin{equation*}
    W_{w}^{^*(P-N)} = \frac{y_{w}^{(P)}/n^{(P)}-y_{w}^{(N)}/n^{(N)}}{y_{w}^{(P)}/n^{(P)}+y_{w}^{(N)}/n^{(N)}}n_{w}
\end{equation*}\todo[color=pink]{leer lowe 2008, understanding wordscores y demostar
this is nearly identical to the difference of proportions measure (correlate at over +0.998)}
