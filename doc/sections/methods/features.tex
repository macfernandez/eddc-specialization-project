Tomando como referencia las t\'ecnicas estad\'isticas descriptas por
\cite{monroe2008fightin}, se implementaron vectorizadores que, a partir de
recibir una lista de documentos, devolviesen una matriz donde cada fila
representase un documento y cada columna, una palabra. Los valores
reflejados en las celdas de esa matriz corresponden a la frecuencia absoluta
de la palabra en el documento en cuesti\'on. Lo que diferencia a
las distintas matrices es el conjunto de palabras seleccionadas
para ser usadas como componentes del vector
que caracteriza a los documentos. Cada vectorizador utiliza
una t\'ecnica estad\'istica determinada para obtener las \textit{N}
palabras m\'as representativas del grupo de votantes a favor y
en contra de la legalizaci\'on del aborto, donde \textit{N} es el n\'umero
entero positivo que indica la cantidad de palabras a considerar.
Los detalles particulares de cada
t\'ecnica pueden encontrarse en la secci\'on \ref{subsubsec-methods-vectorizers}.
\par
Para la implementaci\'on se tom\'o como base el estimador \texttt{CountVectorizer}
de la librer\'ia \textit{scikit-learn}, de modo que las clases u objetos resultantes
contasen con los mismos m\'etodos y, luego, pudieran ser utilizadas en un
flujo de trabajo definido con clases y m\'etodos de la misma librer\'ia.
\par
En primer lugar, se separaron los datos en un conjunto de entrenamiento
y otro de testeo ($80\percentsign$ y $20\percentsign$ de los datos, respectivamente).
En esta divisi\'on se tuvo en cuenta la variable objetivo \texttt{voto} de modo
que los subconjuntos obtenidos reflejasen la misma proporci\'on de votos a favor
y en contra. Asimismo, se us\'o una semilla para garantizar que los resultados
fuesen replicables. La variable objetivo, adem\'as, fue codificada a valores
num\'ericos para poder ser utilizada por los algoritmos de predicci\'on.
\par
Luego, tomando solamente el conjunto de entrenamiento y vectorizando
previamente los datos con cada uno de los vectorizadores desarrollados,
se entren\'o una regresi\'on log\'istica utilizando los par\'ametros por defecto
de la librer\'ia \textit{scikit-learn}
\footnote{Entre los m\'as relevantes para este trabajo, destacamos: regularizaci\'on
o \texttt{penalty=l2}, intensidad inversa de la regularizaci\'on o \texttt{C=1},
ajuste de \textit{bias} o \texttt{fit\_intercept=True} y peso de cada clases
o \texttt{class\_weight=None}.
Para m\'as informaci\'on sobre los par\'ametros por defecto, consultar:
\url{https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html} [\'ultimo acceso: 15--10--2024]}.
En cada entrenamiento se aplic\'o el m\'etodo
de validaci\'on cruzada con 5 subconjuntos: 4 para entrenamiento y 1 para testeo
en cada iteraci\'on.
Para esto, al igual que en la primera divisi\'on, se cuid\'o de realizar
una segmentaci\'on estratificada, utilizar una semilla y
aplicar la misma divisi\'on en cada vectorizador,
de modo que las circunstancias de evaluaci\'on fuesen lo m\'as semejantes posible
y eso permitiese comparar qu\'e vectorizadores conducen a un mejor entrenamiento.
La tabla \ref{table-methods-vectorizers} resume las combinaciones testeadas
en la experimentaci\'on. En todos los casos se generaron vectores de trescientas
dimensiones.

\begin{table}[ht]
\centering
\begin{tabular}{ |c|c|c|c|c| }
    \hline
    T\'ecnica & \textit{Remoci\'on de stopwords} & \textit{Log} & Suavizado \\
    \hline\hline
    Frecuencias absolutas & No & No & No \\
    \hline
    Proporciones & No & No & No \\
    \hline
    Proporciones & S\'i (NLTK) & No & No \\
    \hline
    Proporciones & S\'i (Zipf) & No & No \\
    \hline
    \textit{Ratio} de \textit{odds} & No & No & No \\
    \hline
    \textit{Ratio} de \textit{odds} & No & S\'i (\textit{odds}) & No \\
    \hline
    \textit{Ratio} de \textit{odds} & No & S\'i (\textit{odds}) & S\'i ($0.5$) \\
    \hline
    \textit{TF-IDF} & No & No & No \\
    \hline
    \textit{TF-IDF} & No & S\'i (\textit{IDF}) & No \\
    \hline
    \textit{Word scores} & No & No & No \\
    \hline
\end{tabular}
\caption{T\'ecnicas estad\'isticas testeadas para la vectorizaci\'on de los
discursos utilizados en el entrenamiento del modelo de Regresi\'on Log\'isitca.}
\label{table-methods-vectorizers}
\end{table}

\subsubsection{M\'etodos estad\'isticos utilizados en la vectorizaci\'on de documentos}
\label{subsubsec-methods-vectorizers}

De los m\'etodos aplicados, dos (\textit{TF-IDF} y \textit{Word Scores}) consisten
en algoritmos para la asignaci\'on de puntuaciones o \textit{scores} a las
palabras dentro de un \textit{corpus}. El resto de las t\'ecnicas emplean
m\'etricas estad\'isticas sencillas. Para la aplicaci\'on de estos procedimientos
se utiliz\'o como \textit{token} las lematizaciones de las palabras incluyendo
su categor\'ia gramatical\footnote{Para esto, todos los documentos fueron
pre-procesados de modo tal que cada palabra fue reemplazada por un
\textit{token} construido de la siguiente forma:
$\langle lema \rangle\_ \langle \textit{POS tag} \rangle$, donde
$\langle lema \rangle$ refiere al lema de la palabra y
$\langle \textit{POS tag} \rangle$, a su etiqueta \textit{POS}.}.


\paragraph{Diferencia de frecuencias absolutas}
\label{paragraph-methods-freq-abs}
Este m\'etodo utiliza la diferencia de frecuencias absolutas de los \textit{tokens} entre
los discursos de ambos grupos para extraer el conjunto m\'as representativo de
cada uno:

\begin{equation}
y_t = y_{t}^{(P)}-y_{t}^{(N)}
\end{equation}

donde $y_{t}^{(X)}$ refiere a la cantidad de ocurrencias absolutas del \textit{token}
\textit{t} en los discursos que votaron de manera positiva
(si $X=P$) y negativa (cuando $X=N$).
En los casos en los que $y_{t}$ sea mayor o igual a $0$, el \textit{token} $t$
ser\'a caracter\'istico del discurso de los senadores que votaron a favor de
la legalizaci\'on. En caso contrario, ser\'a caracter\'istico de quienes votaron
en contra.
De este modo, se seleccionaron las trescientas dimensiones m\'as representativas de
ambos grupos cuidando que hubiese igual cantidad de \textit{tokens} para cada uno.
Un procedimiento an\'alogo es aplicado en el resto de las t\'ecnicas pero utilizando
un criterio de contrastaci\'on distinto en cada caso.

\paragraph{Diferencia de proporciones}
\label{paragraph-methods-proportions}
El segundo contraste utiliza la proporci\'on de \textit{tokens}
en cada conjunto de discursos.

\begin{equation}
    f_t = f_{t}^{(P)}-f_{t}^{(N)}
\end{equation}


donde $f_{t}^{(X)}$ se define como $y_{t}^{(X)} / n^{(X)}$, siendo $y_{t}^{(X)}$
la frecuencia absoluta del \textit{token} $t$ en el cojunto los discursos de $X$
y $n^{(X)}$, la cantidad de \textit{tokens} totales en esos discursos.

\subparagraph{Remoci\'on de \textit{stopwords}}
\label{paragraph-methods-proportions-stopwords}
Tambi\'en se intent\'o calcular la diferencia de proporciones removiendo
previamente los \textit{tokens} considerados \textit{stopwords}.
Para esto se probaron dos alternativas.
Por un lado, se utiliz\'o el conjunto predefinido de la librer\'ia \textit{NLTK}
\footnote{\url{https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/stopwords.zip} [\'ultimo acceso: 15--10--2024]}
y, por otro, la Ley de Zipf.
Este \'ultimo procedimiento consiste en calcular la frecuencia absoluta de aparici\'on de
cada \textit{token} y, en virtud de ella, establecer un \textit{ranking}
orden\'andolos de manera decreciente.
Luego, se contrasta dicha frecuencia con el orden asignado y se define
un umbral a partir del cual una palabra es considerada \textit{stopword}.
Resulta pertinente notar que, dado que la frecuencia absoluta de aparici\'on
de una palabra se determina en relaci\'on al conjunto de documentos en el cual
esta frecuencia es medida, las diferentes iteraciones de la validaci\'on cruzada
generaron distintos conjuntos de \textit{stopwrods}.
En todas las iteraciones se removieron $100$ \textit{tokens} seleccionados
con este m\'etodo. Este n\'umero fue seleccionado tras realizar un an\'alisis del
conjunto de datos completo como primera aproximaci\'on general (ver secci\'on
\ref{appendix-plots-zipf-law}.).

\paragraph{\textit{Ratio} de \textit{Odds}}
Se utilizan \textit{odds} de ambos conjuntos de discursos como
t\'ecnica de selecci\'on. En este caso, los \textit{tokens} m\'as caracter\'isticos
no resultan de una diferencia sino de un cociente:

\begin{equation}
    O_{t} = \frac{O_{t}^{(P)}}{O_{t}^{(N)}}
\end{equation}

aqu\'i, $O_{t}^{(X)}$ consiste en $f_{t}^{(X)}/(1-f_{t}^{(X)})$.
Otra diferencia entre esta t\'ecnica y el resto es que aqu\'i el punto de
corte para considerar a un \textit{token} como caracter\'istico de
determinado grupo de discursos no es el $0$ sino el $1$, por la misma
definici\'on de la funci\'on.

\paragraph{\textit{Ratio} \textit{Log-odds}}
Sobre los \textit{odds}, tambi\'en se calcul\'o el \textit{ratio log-odds}.
Para este c\'alculo se utiliz\'o el logaritmo natural: $\ln{O_t}$.
Asimismo, se realiz\'o un suavizado sobre las frecuencias en los casos
en los que los \textit{tokens} solo se encontraban en uno de los grupos
discursivos. Dicho suavizado consisti\'o en agregar un peso predefinido a las
frecuencias relativas, $\tilde{f}^{i}_{w} = f^{i}_{w}+\epsilon$, y tomar
este valor en el c\'alculo del \textit{ratio}. Siguiendo a \cite{monroe2008fightin},
se utiliz\'o $\epsilon=0.5$.

\paragraph{\textit{TF-IDF}}
\label{paragraph-methods-tfidf}
Si bien la librer\'ia \textit{scikit-learn} cuenta con un vectorizador
que permite calcular los \textit{scores} de \textit{TF-IDF}
\footnote{Para m\'as informaci\'on sobre las f\'ormulas
implementadas por \textit{scikit-learn}, visitar:
\url{https://scikit-learn.org/stable/modules/feature_extraction.html\#tfidf-term-weighting} [\'ultimo acceso: 15--10--2024].},
dado que la f\'ormula propuesta por \Citeauthor{monroe2008fightin} plantea
algunas modificaciones a dicha implementaci\'on, se decidi\'o
desarrollar un vectorizador propio para obtener un resultado
m\'as aproximado al descripto por los autores, quienes indican haber usado
dos f\'ormulas diferentes: ambas emplean la frecuencia
de t\'ermino natural y sin normalizar, pero una con la frecuencia de documentos
con logaritmo (f\'ormula \ref{equation-tfidf-ntn}) y la otra, con la frecuencia
de documentos natural (f\'ormula \ref{equation-tfidf-nnn}).

\begin{equation}
\label{equation-tfidf-ntn}
    tf.idf_{t}^{(X)}(ntn) = f_{t}^{(X)} \times \ln\bigg({\frac{1}{df_{t}}}\bigg)
\end{equation}

\begin{equation}
\label{equation-tfidf-nnn}
    tf.idf_{t}^{(X)}(nnn) = \frac{f_{t}^{(X)}}{df_{t}}
\end{equation}

donde $f_{t}^{X}$ refiere a la proporci\'on del \textit{token} $t$
en el discurso perteneciente a $X$
(con $X \in \lbrace P,N \rbrace$) y $df_t$, a la cantidad
de documentos totales en los que ese \textit{token} aparece.
\par
Lo que los autores no mencionan es qu\'e tipo de comparaci\'on realizan
entre los \textit{tokens} pertenecientes a los distintos discursos.
Siguiendo los procedimientos anteriores, en este trabajo se opt\'o por usar
la sustracci\'on, por lo que:

\begin{equation*}
    tf.idf_{t} = tf.idf_{t}^{P}-tf.idf_{t}^{N}
\end{equation*}

\paragraph{\textit{Word Scores}}
\label{paragraph-methods-wordscores}
Por \'ultimo, este procedimiento le asigna un peso a los \textit{tokens}
a partir del siguiente c\'alculo:

\begin{equation*}
    W_{t}^{^*(P-N)} = \frac{
        y_{t}^{(P)}/n^{(P)}-y_{t}^{(N)}/n^{(N)}
        }{
            y_{t}^{(P)}/n^{(P)}+y_{t}^{(N)}/n^{(N)}
        }n_{t}
\end{equation*}

donde $y_{t}^{(X)}$ refiere a la frecuencia absoluta
del grupo $X$ y $n^{(X)}$, a la cantidad de \textit{tokens} totales en $X$.
